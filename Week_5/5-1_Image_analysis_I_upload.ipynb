{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c0615af-791e-47b2-b066-782af0c8829a",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 40px; margin-bottom: 0px;\">5.1 Python image analysis (I)</h1>\n",
    "\n",
    "<hr style=\"margin-left: 0px; border: 0.25px solid; border-color: #000000; width: 800px;\"></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72dfafab-6ce6-4455-9ebd-25cafdf35ed7",
   "metadata": {},
   "source": [
    "In today's lecture, we went over how images are expressed as either 2D matrices (single channel) or 3D matrices (multi-channel or true-color) with each pixel as an element in the matrix containing a numerical value representing the intensity of light. We're all familiar with using ImageJ for analyzing images, but here, we'll use Python to perform many of the same analyses that we can do in ImageJ. \n",
    "\n",
    "For today's notebook, we'll explore how images are represented on our computers in order to learn process our images. To do this, we'll make use of a package called <code>scikit-image</code>, which is an image processing package that uses numpy arrays. <a href=\"https://scikit-image.org/\" rel=\"noopener noreferrer\"><u>Information, including documentation, on <code>scikit-image</code> can be found here.</u></a>\n",
    "\n",
    "<strong>Learning objectives:</strong>\n",
    "<ul>\n",
    "    <li>Learn how to import images in a Python notebook</li>\n",
    "    <li>Understand how images are represented as a matrix</li>\n",
    "    <li>Learn how to display images in a Python notebook</li>\n",
    "    <li>Learn how to process images</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b0ea86-83f4-4edc-a07e-fe0cc90f9b5e",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 40px; margin-bottom: 0px;\">Install <code>scikit-image</code> package</h1>\n",
    "\n",
    "<hr style=\"margin-left: 0px; border: 0.25px solid; border-color: #000000; width: 600px;\"></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09deeb31-8761-40b2-aae9-76947fd92845",
   "metadata": {},
   "source": [
    "Recall from MCB201A notebook 16-2 that we can install packages that we need, and for today's lesson, we will make use of the <code>scikit-image</code> package, which is not currently in our virtual environment. \n",
    "\n",
    "To install the <code>scikit-image</code> package, we can make use of Terminal and the <code>pip</code> command to install our package.\n",
    "\n",
    "<pre style=\"width: 350px; margin-top: 15px; margin-bottom: 15px; color: #000000; background-color: #EEEEEE; border: 1px solid; border-color: #AAAAAA; padding: 10px; border-radius: 15px; font-size: 12px;\">pip install scikit-image</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdaa999-9fab-4d6a-995b-c9abc4344deb",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 40px; margin-bottom: 0px;\">Import packages for today</h1>\n",
    "\n",
    "<hr style=\"margin-left: 0px; border: 0.25px solid; border-color: #000000; width: 600px;\"></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f60911-94a4-4a89-aa40-2e17752ffbd6",
   "metadata": {},
   "source": [
    "After our package is successfully installed, let's import all the packages we'll use for today's notebook. \n",
    "\n",
    "We'll make use of our usual packages:\n",
    "\n",
    "<ul>\n",
    "    <li><code>numpy</code></li>\n",
    "    <li><code>pandas</code></li>\n",
    "    <li><code>matplotlib.pyplot</code></li>\n",
    "    <li><code>seaborn</code></li>\n",
    "    <li><code>os</code></li>\n",
    "</ul>\n",
    "\n",
    "And we'll make use of two packages that we haven't used before:\n",
    "\n",
    "<ul>\n",
    "    <li><code>skimage</code></li>\n",
    "    <li><code>scipy.ndimage</code></li>\n",
    "</ul>\n",
    "\n",
    "These two packages are useful for processing images in Python to get them ready for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c0cfb8-1402-400c-95ff-40e65c3f7851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import skimage as ski\n",
    "import scipy.ndimage as ndi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de384bc-ed69-4b8f-b4ed-eadc74ca51d1",
   "metadata": {},
   "source": [
    "Let's take a look to see if our <code>ski</code> package is the proper version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcbcd03-c980-4085-abdc-d04df9585d8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d7edec80-d682-43d2-b239-c6fc1439b1c4",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 40px; margin-bottom: 0px;\">Importing images into Python</h1>\n",
    "\n",
    "<hr style=\"margin-left: 0px; border: 0.25px solid; border-color: #000000; width: 600px;\"></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72f9b4b-7e81-4ffb-bbcc-02d6c44f92d8",
   "metadata": {},
   "source": [
    "To work with our images in Python, we'll first need to import them like we would with any normal dataset. However, rather than using the panadas package and <code>pd.read_csv()</code> to import our file, we'll be using matplotlib's function <code>plt.imread()</code>. This function will read the data contained within an image file into a multidimensional array that we can then work with in Python.\n",
    "\n",
    "Let's give it a try with a grayscale image of Liebchen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b08b5a6-994b-4459-b8e4-bbdd7a39800b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4aea7e64-242f-489c-ba72-759a8ac915de",
   "metadata": {},
   "source": [
    "As you recall from lecture, an 8-bit grayscale image is represented as a 2D matrix containing values from 0 (black) to 255 (white). We can take a look at a slice of our imported image to see how the image file is understood by Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295a49db-a822-4ab9-8c92-b8b6435ad1d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a71fcef-f949-471f-8c09-dcde5ad1dee5",
   "metadata": {},
   "source": [
    "What you can see is that each pixel is represented by a value that represents its grayscale value. In this case, our image file is an 8bit grayscale image, so we have a single 2D array where the values can span 0-255. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a1edf6-c837-435c-98e1-4c10a7ff890e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "342a424c-e409-44c7-b85f-c7b538920893",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 40px; margin-bottom: 0px;\">Exploring grayscale images in Python</h1>\n",
    "\n",
    "<hr style=\"margin-left: 0px; border: 0.25px solid; border-color: #000000; width: 700px;\"></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa78010-27f8-4d3e-9b3c-898ec56b68b6",
   "metadata": {},
   "source": [
    "To render this image in our Python notebook, we can make use of matplotlib, but this time instead of using this library to display our plots, we'll use it to display our images. To do this, we'll make use of the <code>plt.imshow()</code> function. <a href=\"https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.imshow.html\" rel=\"noopener noreferrer\" target=\"_blank\"><u>Documentation for <code>plt.imshow()</code> can be found here.</u></a> \n",
    "\n",
    "If we dig into the documentation, we'll see that we can pass a 2D numpy array to the function along with additional arguments to adjust how the image will be displayed/rendered. First, let's display our image with the default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4d4e72-2cf8-4486-8cf1-59eac510c0f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98c44f7f-5424-4aa6-bb93-ebcdedc44475",
   "metadata": {},
   "source": [
    "You can see that by default, the colormap <code>cmap</code> will be a colormap called <a href=\"https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html\" rel=\"noopener noreferrer\"><u>'viridis', which was developed to improve readability and accessibility of quantitative data visualizations, such as as heatmaps</u></a>. To display our image as a grayscale image, we can pass <code>'gray'</code> to the <code>cmap</code> parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878ac759-e897-4857-a142-9f07836b0c61",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "69328fe6-1d23-4bdd-b607-5c13e65af4db",
   "metadata": {},
   "source": [
    "We can also specify what value corresponds to pure black and what value corresponds to pure white using the <code>vmin</code> and <code>vmax</code> parameters, respectively. You'll want to keep in mind that the underlying data isn't changed by adjusting the <code>vmin</code> and the <code>vmax</code>. These parameters change how the image is displayed/rendered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569f8e2f-aa1e-468d-ae7c-b3d11fc73ad5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68959f53-d0b8-461e-b78e-43c08e662b39",
   "metadata": {},
   "source": [
    "Since our images are 2D arrays, we can also use slice notation to pull out portions of our image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903e0ade-b01d-4c1b-9b95-002528c0aef3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39b737f5-bc0a-4140-abf9-d40ebde1d76c",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 40px; margin-bottom: 0px;\">Exploring color images in Python</h1>\n",
    "\n",
    "<hr style=\"margin-left: 0px; border: 0.25px solid; border-color: #000000; width: 700px;\"></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b1358c-c843-416d-88bb-3ddf8acd7876",
   "metadata": {},
   "source": [
    "Recall from lecture that true-color images are represented by three channels that are overlaid one on top of the other to display the final color. Let's import a color image and then explore how its represented as a 3D numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bec6ab8-e0dc-4374-b4ad-37ca3ffc3275",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0f3ba6e3-9d2b-4eb2-9a61-9c52c3fa3c32",
   "metadata": {},
   "source": [
    "Recall from lecture that true color images are represented by a stack of three 2D arrays, where each 2D array corresponds to an individual channel, either red, green, or blue. Let's take a look at how the array itself looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1ee488-6406-4643-9c01-137fd1610468",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b142bc60-fe6b-4e69-bf8b-6e2a88d8ccbc",
   "metadata": {},
   "source": [
    "What you can see is that based on the syntax of our 3D matrix, it is a bit difficult to exactly tease out complete information on each channel by simply looking at the object in its entirety. From earlier, we know from our experience with 2D objects, like pandas DataFrames, that the dimensions are ordered as <code>&lbrack;a, b&rbrack;</code> with the first dimension <code>a</code> corresponding to the number of rows (height of our image), and the second dimension <code>b</code> to the number of columns (width of our image). So then for our 3D image, which is just our 2D arrays stacked one on top of another along a third dimension <code>c</code>:\n",
    "\n",
    "<p style=\"font-size: 16px; text-align: center;\"><strong>True-color image as 3D array</strong></p>\n",
    "<img src=\"./ref-images/array-explanation.jpg\" style=\"height: 300px; margin: auto\"/>\n",
    "\n",
    "So then, our true-color image can be represented as the 3D array <code>&lbrack;a, b, c&rbrack;</code>.\n",
    "\n",
    "This means that for a 3-channel true-color image, the values for c will be either <code>0</code>, <code>1</code>, or <code>2</code>, representing grayscale intensity values for red, green, and blue, respectively. Let's pull out a single row from our true-color image to look at the RGB values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4151fffa-9590-4efc-b06c-213e19c536f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "849a3ce6-b714-4bc9-85f8-4970ee05a255",
   "metadata": {},
   "source": [
    "And if we wanted all three channels of a single pixel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24a034f-9377-4b48-9e71-9786d39d53fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c5f9589-e4f4-4db0-8d57-0e8c8a190e24",
   "metadata": {},
   "source": [
    "So if we wanted a single channel, we can specify that as the third dimension. For our single pixel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae5bb6c-2202-4c1e-a10c-793befaeb866",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93f12791-08f3-4c40-8197-169800d8390b",
   "metadata": {},
   "source": [
    "If we wanted the full 2D array for a single channel, we'll need to specify that we want all values from the first two dimensions, and just a single value for our third dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9781181-0aa6-43e5-a2b1-210645d9ec11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d8538aab-5708-4792-adc2-c910c6a9d147",
   "metadata": {},
   "source": [
    "We can then plot a single channel using the same function we used before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3377c0-baa7-4c2b-8f64-394e5ddcc46e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "de83b909-cbe4-485d-ad4d-520910642041",
   "metadata": {},
   "source": [
    "Let's plot all three channels in grayscale side-by-side using <code>plt.implot()</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71005317-a47c-4d74-b91c-1d73e9caa6b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77a904d1-ef22-4f65-b520-989773c72375",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 40px; margin-bottom: 0px;\">Assembling a composite image</h1>\n",
    "\n",
    "<hr style=\"margin-left: 0px; border: 0.25px solid; border-color: #000000; width: 700px;\"></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b3608d-29db-4361-b0be-56c93c49468b",
   "metadata": {},
   "source": [
    "More often than not, we're imaging our samples one channel at a time, so if we want to display our images as a composite, we need to construct a 3D array out of our separate 2D arrays. We can do this by using the <code>np.dstack()</code> function, which will stack/concatenate our arrays along the third axis. <a href=\"https://numpy.org/doc/stable/reference/generated/numpy.dstack.html\" rel=\"noopener noreferrer\"><u>Documentation for <code>np.dstack()</code> can be found here.</u></a>\n",
    "\n",
    "Recall from earlier that the position of our 2D arrays in the 3rd dimension, specifies what channel it's in, so it's important to keep this in mind so that you know what channel(s) you are pulling out or compositing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67bf098-b63f-416e-9e6f-3e6b21ebff26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf224de5-bc88-43d9-9510-18ceba444fa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "651f650a-5d9a-4272-a004-3c32e675c023",
   "metadata": {},
   "source": [
    "For example, if we mix up the channels, our true-color image won't be accurate to what we want it to show."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93eb27d8-960a-4bd3-9c67-8055b4eff2e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cf7e4d2e-48b9-429c-9827-4d196ebe4ba8",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 40px; margin-bottom: 0px;\">Pseudocolor an image</h1>\n",
    "\n",
    "<hr style=\"margin-left: 0px; border: 0.25px solid; border-color: #000000; width: 700px;\"></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3b70dc-1030-4641-95db-86ffb01c977c",
   "metadata": {},
   "source": [
    "We can take what we know about how true-color images are built up as a composite of three channels to also pseudocolor our images. If we repeat a single channel in all three channels, we will end up again with a grayscale image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6930a16-acbe-418a-a6d0-e3e68c4325c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7640a8b7-9483-4a8d-9311-cddeabf2e4ac",
   "metadata": {},
   "source": [
    "We can then take what we know about mathematical operations on arrays to then adjust the values for each channel. If we want to then pseudocolor our grayscale image red, we can zero-out the green and blue channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907db103-0312-451c-8851-d4be6535adc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4adc3f12-1954-4b74-88b5-e9dbc46ff0e8",
   "metadata": {},
   "source": [
    "We can also achieve the same result by setting up a 2D array of zeros of the same dimensions as our image, and placing it in the positions corresponding to the channels we don't want to use. To set up an array of zeros that are the same dimensions as another object, we can make use of the <code>np.zeros_like()</code> function. <a href=\"https://numpy.org/doc/stable/reference/generated/numpy.zeros_like.html\" rel=\"noopener noreferrer\"><u>Documentation for <code>np.zeros_like()</code> can be found here.</u></a>\n",
    "\n",
    "This function takes an object as an argument and then will initialize an array of zeros that matches the shape of the object that it was given, so if we provide it with one of our 2D arrays corresponding to our image, it will generate an array of zeros of the same shape as our image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ff5b65-bdb1-4031-9793-cc5bfcf76b04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6479c6b5-81b9-4673-9ffe-d5909007eeb6",
   "metadata": {},
   "source": [
    "Let's then assemble the three channels with two of them assigned our array of zeros and generate an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92eb39b7-9fe6-4ea5-afab-731f10761835",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d2e50992-4513-4f39-9aa4-13aeedf73bdd",
   "metadata": {},
   "source": [
    "We can also mess with the numbers to pseudocolor our image in a color that isn't exactly red, green, or blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae84b559-607e-478f-9eb6-f684c6db2e8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c24c468e-1cc4-44d6-8718-d4a64845f7f6",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 40px; margin-bottom: 0px;\">Exporting processed images</h1>\n",
    "\n",
    "<hr style=\"margin-left: 0px; border: 0.25px solid; border-color: #000000; width: 700px;\"></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c98f5a-e8ab-4fc1-85d6-64e2a34581b9",
   "metadata": {},
   "source": [
    "Since we're using matplotlib to render our images, we can make use of our usual <code>plt.savefig()</code> or <code>fig.savefig()</code> functions to export our processed images. This time, instead of specifying our images with the extension <code>.pdf</code>, we can specify <code>.jpg</code> or another image extension, and our image will be exported as that file type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989a9acc-fd92-4bf6-8996-ec174ac7fb17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54a6d175-68ed-4cc4-8925-fd1bbb88819f",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 40px; margin-bottom: 0px;\">Exercise #1: Import fluorescence data</h1>\n",
    "\n",
    "<hr style=\"margin-left: 0px; border: 0.25px solid; border-color: #000000; width: 850px;\"></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565ac7df-84e9-43ae-ae70-db44222e78de",
   "metadata": {},
   "source": [
    "For the first set of exercises, let's take what we've learned and then apply it to our fluorescence data from MCB201A. First we'll import our fluorescence data files under the <code>data</code> subdirectory. We'll import all the files and apply what we know about how images are represented to process our fluorescence data and analyze them to extract quantitative data.\n",
    "\n",
    "You can load it in one by one or if you remember from when we worked with multiple files for statistical analysis, see if you can apply that here to load in the data for today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac30af7-abf3-4812-a92c-fa3f1f7a85fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fe1fb842-cf8d-4b95-89f1-4f0dd7e210e5",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 40px; margin-bottom: 0px;\">Exercise #2: Pseudocolor channels</h1>\n",
    "\n",
    "<hr style=\"margin-left: 0px; border: 0.25px solid; border-color: #000000; width: 850px;\"></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e0b35c-1b9a-4b46-9906-a5ed46b7ddd8",
   "metadata": {},
   "source": [
    "See if you can take what you've learned to then pseudocolor an individual channel from our set of images. Select either the no serum or serum stimulation condition and psuedocolor each channel for that treatment condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd4e355-9d26-44cc-b9c7-7da2a0097ca3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5b53cf19-4155-4aea-b1bb-fdf646d244a1",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 40px; margin-bottom: 0px;\">Exercise #3: Render three-channel composite images</h1>\n",
    "\n",
    "<hr style=\"margin-left: 0px; border: 0.25px solid; border-color: #000000; width: 950px;\"></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2ad369-2ad6-4b42-b26a-e6d48700188a",
   "metadata": {},
   "source": [
    "For this exercise, create three-channel composite images for both of our treatment conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2348b0d0-779c-41f6-9e94-56ec2ef5982d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0cd151a1-931d-4d13-bd9c-3928ade91490",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 40px; margin-bottom: 0px;\">Exercise #4: Identify a \"good\" threshold</h1>\n",
    "\n",
    "<hr style=\"margin-left: 0px; border: 0.25px solid; border-color: #000000; width: 850px;\"></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b7ab9b-9794-4413-abae-3a187f3dfe67",
   "metadata": {},
   "source": [
    "For this exercise, we'll take a look at the underlying values of our DAPI images to identify what value we can set as a potentially \"good\" threshold to segment our nuclei from our background. \n",
    "\n",
    "We'll need to first take a look at the distribution of all pixel intensities for our DAPI images, and since our DAPI images are a 2D array, it won't plot neatly as is. So you'll first need to make use of the <code>np.ndarray.flatten()</code> to flatten your 2D array into a single dimension, so that it can be used to plot a simple histogram.\n",
    "\n",
    "Plot a histogram displaying the distribution of values of our two DAPI channels, and plot each channel as its own distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce87aef-8022-4698-8851-5724032327b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "17120e09-583e-41af-a8ad-eefbe1eca23a",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 40px; margin-bottom: 0px;\">Exercise #5: Segment nuclei using your threshold</h1>\n",
    "\n",
    "<hr style=\"margin-left: 0px; border: 0.25px solid; border-color: #000000; width: 900px;\"></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199997ac-3018-4fb5-9fc6-9a512a20bbb5",
   "metadata": {},
   "source": [
    "Now that you've identified a good spot to set your threshold, see if you can make use of a conditional statement to threshold your DAPI images, thereby converting it into binary images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc23f87b-30eb-4142-be1c-7aa36ffef8f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "38ba7c21-0fc4-4b58-b405-2b8f0a3c29f8",
   "metadata": {},
   "source": [
    "Take a look at the array values now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3802911-1011-427e-8a6d-b99c20104d33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0530e484-423c-4d58-abf8-b94250cd64d6",
   "metadata": {},
   "source": [
    "You should be able to see now that your thresholded array contains booleans. This is because we applied our conditional statement to each element in our 2D arrays, resulting in a boolean output corresponding to whether or not each element in the array met that condition. Now render your binary images side by side to see how the nuclei have been segmented for both images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca65177-35e0-4efb-bdc7-a8d9689418cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "90f14986-89f9-4f2e-8bc3-2ecd4466610e",
   "metadata": {},
   "source": [
    "There are also many other ways to calculate thresholds, and when we reconvene here, we'll go over additional ways to threshold our images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cd7dd0-89b9-4a5f-bddd-d340d8bc2078",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cca3b28d-3140-407d-94c3-437fd34b96c9",
   "metadata": {},
   "source": [
    "If we then wanted to use one of these algorithms to threshold our nuclei, we can call them up from the <code>scikit-image</code> package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16925541-eb06-44b9-bd76-b0b2099eb797",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b5124fb2-47f9-4afa-b4c7-7d19b6f911c4",
   "metadata": {},
   "source": [
    "We can also apply a local threshold, where if our background signal was uneven in our image, we can take into account the local background to segment our images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78200a9a-a657-4aa4-9714-0d7383b25081",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "88e4c73c-a14c-49f0-b5fa-923f69d30717",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 40px; margin-bottom: 0px;\">Fill holes in a thresholded image</h1>\n",
    "\n",
    "<hr style=\"margin-left: 0px; border: 0.25px solid; border-color: #000000; width: 600px;\"></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e926cd-d96f-47df-9459-8f8b0ecb0691",
   "metadata": {},
   "source": [
    "Sometimes our thresholded image isn't exactly what we want it to be, and in the case of a few of our nuclei, we have some holes that will be helpful to fill so that we capture the entirety of each nuclei. To do this, we can make use of the <code>ndi.fill_binary_holes()</code> function, which is a quick way to fill in holes in our multidimensional binary array. <a href=\"https://docs.scipy.org/doc/scipy/reference/generated/scipy.ndimage.binary_fill_holes.html\" rel=\"noopener noreferrer\"><u>Documentation for <code>ndi.fill_binary_holes()</code> can be found here.</u></a>\n",
    "\n",
    "Let's first take a look at one of our thresholded images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6fe00f-a76f-4358-abfc-25de9a2c9bca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1292949b-d11a-4c44-96cb-203e43afbc9d",
   "metadata": {},
   "source": [
    "We can see that there are holes within our segmented nuclei, and we can fill them in using <code>ndi.binary_fill_holes()</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570ae4fa-7278-493e-8d94-9cce271d2111",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b2d8addc-b9f2-42db-b78f-0cefef6e083f",
   "metadata": {},
   "source": [
    "Another way, would be to do a dilation followed by an erosion, which will help to fill in gaps while limiting how much we expand our region of interest. To do this, we'll make use of some functions within scikit-image:\n",
    "\n",
    "<ul>\n",
    "    <li><code>ski.morphology.disk()</code> - this creates a binary circle for us to use as a footprint in our dilation and erosion. <a href=\"https://scikit-image.org/docs/0.25.x/api/skimage.morphology.html#skimage.morphology.disk\" rel=\"noopener noreferrer\"><u>Documentation is here.</u></a></li>\n",
    "    <li><code>ski.morphology.dilation()</code> - this will increase the size of bright regions (our ROI). <a href=\"https://scikit-image.org/docs/0.25.x/auto_examples/applications/plot_morphology.html#dilation\" rel=\"noopener noreferrer\"><u>Documentation is here.</u></a></li>\n",
    "    <li><code>ski.morphology.erosion()</code> - this will increase the size of dark regions (our background). <a href=\"https://scikit-image.org/docs/0.25.x/auto_examples/applications/plot_morphology.html#erosion\" rel=\"noopener noreferrer\"><u>Documentation is here.</u></a></li>\n",
    "    <li><code>ski.morphology.closing()</code> - this will performs a dilation followed by an erosion, and can be another way to achieve a similar result. <a href=\"https://scikit-image.org/docs/0.25.x/auto_examples/applications/plot_morphology.html#closing\" rel=\"noopener noreferrer\"><u>Documentation is here.</u></a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ead6646-185a-4175-8ca2-f1520b86ff88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb89e6f4-e3b4-4a17-8fc0-4705cbd042d7",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 40px; margin-bottom: 0px;\">Exercise #6: Use binary threshold as a mask</h1>\n",
    "\n",
    "<hr style=\"margin-left: 0px; border: 0.25px solid; border-color: #000000; width: 850px;\"></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed582ef8-6bcb-47b4-9d6a-62fde8408901",
   "metadata": {},
   "source": [
    "For this exercise, see if you can take what you now know about working with images and mathematical operations on arrays to use your segmented nuclei as a mask to look at just our region of interest (ROI) in the green and red channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6241c36-efe2-4e59-b79c-8b50c715513b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21bbb46a-70ce-49c7-9a2f-87afd74ea87a",
   "metadata": {},
   "source": [
    "<h1 style=\"font-size: 40px; margin-bottom: 0px;\">Exercise #7: Segment cells and/or cytoplasm</h1>\n",
    "\n",
    "<hr style=\"margin-left: 0px; border: 0.25px solid; border-color: #000000; width: 850px;\"></hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1592f29a-9cc9-425a-979f-db9511d761b0",
   "metadata": {},
   "source": [
    "For this exercise, practice applying what you've learned so far to image analysis to pull cells as our ROI or just the cytoplasm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc47f7b-dd4d-4d20-8607-b41f2618db7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
